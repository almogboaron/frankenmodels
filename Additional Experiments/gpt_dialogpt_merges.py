# -*- coding: utf-8 -*-
"""got dialogpt merges.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LYPUYPyGjdcUcmA-EZvCdVj-G-kdoztI
"""

!pip install transformers torch

!pip install datasets
!pip install rouge-score

import torch
from transformers import GPT2LMHeadModel, AutoTokenizer
from datasets import load_dataset
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
nltk.download("punkt")
nltk.download("punkt_tab")

smoothie = SmoothingFunction().method4

# List of models to compare
models = {
    "GPT-2": "openai-community/gpt2",
    "DialoGPT-small": "microsoft/DialoGPT-small",
    "DialoGPT-gpt2-TA": "amitom/DialoGPT-gpt2-TA",
    "DialoGPT-gpt2-SLERP": "amitom/gpt2-DiabloGPT-SLERP",
    "Dialogpt-gpt2-Ties": "amitom/DialoGPT-gpt2-TIES",
    "Dialogpt-gpt2-Passthrough": "amitom/DialoGPT-gpt2-Passthrough"
}


print("Loading dataset...")
dataset = load_dataset("daily_dialog", split="validation", trust_remote_code=True)
text_samples = dataset["dialog"][:200]

flattened_text_samples = [sentence for dialogue in text_samples for sentence in dialogue]


def calculate_perplexity(model, tokenizer, sentences, max_tokens=1000):
    encodings = tokenizer("\n".join(sentences), return_tensors="pt", truncation=True, max_length=max_tokens)
    input_ids = encodings["input_ids"]

    with torch.no_grad():
        outputs = model(input_ids, labels=input_ids)
        loss = outputs.loss

    total_loss = loss.item() * input_ids.shape[1]
    total_tokens = input_ids.shape[1]
    avg_loss = total_loss / total_tokens
    perplexity = torch.exp(torch.tensor(avg_loss))
    return perplexity.item()

def generate_response(model, tokenizer, prompt, max_new_tokens=30):
    input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).input_ids
    output_ids = model.generate(input_ids, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id, temperature=0.7, top_p=0.9)
    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return output_text[len(prompt):].strip()

def evaluate_bleu(model, tokenizer, dialogues, max_samples=100):
    scores = []
    for dialogue in dialogues[:max_samples]:
        if len(dialogue) < 2:
            continue
        context = " ".join(dialogue[:-1])
        reference = nltk.word_tokenize(dialogue[-1])
        generated = generate_response(model, tokenizer, context)
        hypothesis = nltk.word_tokenize(generated)
        score = sentence_bleu([reference], hypothesis, smoothing_function=smoothie)
        scores.append(score)
    return sum(scores) / len(scores)

def evaluate_mmlu(model, tokenizer, mmlu_dataset, max_samples=85):
    correct_answers = 0
    for sample in mmlu_dataset.select(range(max_samples)):
        question = sample['question']
        choices = sample['choices']
        correct_answer_index = sample['answer']

        prompt = f"Question: {question}\nChoices: {', '.join(choices)}\nAnswer:"

        generated_answer = generate_response(model, tokenizer, prompt)

        print(f"Question: {question}")
        print(f"Generated: {generated_answer.strip()}")
        print(f"Correct: {choices[correct_answer_index]}")

        if generated_answer.strip().lower() == choices[correct_answer_index].lower():
            correct_answers += 1

    accuracy = correct_answers / max_samples
    return accuracy

def evaluate_rouge(model, tokenizer, dialogues, max_samples=100):
    scores = []
    for dialogue in dialogues[:max_samples]:
        if len(dialogue) < 2:
            continue
        context = " ".join(dialogue[:-1])
        reference = dialogue[-1]
        generated = generate_response(model, tokenizer, context)

        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        score = scorer.score(reference, generated)
        scores.append(score)

    avg_rouge = {
        'rouge1': sum([score['rouge1'].fmeasure for score in scores]) / len(scores),
        'rouge2': sum([score['rouge2'].fmeasure for score in scores]) / len(scores),
        'rougeL': sum([score['rougeL'].fmeasure for score in scores]) / len(scores)
    }
    return avg_rouge

rouge_scores = {}
perplexity_scores = {}
bleu_scores = {}

for name, model_path in models.items():
    print(f"\nLoading {name}...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
    model = GPT2LMHeadModel.from_pretrained(model_path)
    model.eval()

    # Perplexity
    print("Calculating perplexity...")
    perplexity = calculate_perplexity(model, tokenizer, flattened_text_samples)
    perplexity_scores[name] = perplexity

    # BLEU
    print("Calculating BLEU score...")
    bleu = evaluate_bleu(model, tokenizer, text_samples)
    bleu_scores[name] = bleu

    # ROUGE
    print("Calculating ROUGE score...")
    rouge = evaluate_rouge(model, tokenizer, text_samples)
    rouge_scores[name] = rouge

# Output results
print("\nModel Evaluation Results:")
for name in models:
    print(f"{name}: Perplexity = {perplexity_scores[name]:.2f} | BLEU = {bleu_scores[name]:.4f} | ROUGE-1 = {rouge_scores[name]['rouge1']:.4f} | ROUGE-2 = {rouge_scores[name]['rouge2']:.4f} | ROUGE-L = {rouge_scores[name]['rougeL']:.4f}")

new_models = {
    "DialoGPT gpt2 Ties DialoGPT Heavy" : "amitom/DialoGPT-gpt2-TIES-DialoHeavy",
    "DialoGPT gpt2 Ties gpt2 Heavy" : "amitom/DialoGPT-gpt2-TIES-gpt2Heavy"
}



for name, model_path in new_models.items():
    print(f"\nLoading {name}...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
    model = GPT2LMHeadModel.from_pretrained(model_path)
    model.eval()

    # Perplexity
    print("Calculating perplexity...")
    perplexity = calculate_perplexity(model, tokenizer, flattened_text_samples)
    perplexity_scores[name] = perplexity

    # BLEU
    print("Calculating BLEU score...")
    bleu = evaluate_bleu(model, tokenizer, text_samples)
    bleu_scores[name] = bleu

    # ROUGE
    print("Calculating ROUGE score...")
    rouge = evaluate_rouge(model, tokenizer, text_samples)
    rouge_scores[name] = rouge

combined_models = {**models, **new_models}

print("\nModel Evaluation Results:")
for name in combined_models:
    print(f"{name}: Perplexity = {perplexity_scores[name]:.2f} | BLEU = {bleu_scores[name]:.4f} | ROUGE-1 = {rouge_scores[name]['rouge1']:.4f} | ROUGE-2 = {rouge_scores[name]['rouge2']:.4f} | ROUGE-L = {rouge_scores[name]['rougeL']:.4f}")

import matplotlib.pyplot as plt
import numpy as np

# Model names
model_names = [
    "GPT-2", "DialoGPT-small", "DialoGPT-gpt2-TA", "DialoGPT-gpt2-SLERP", "Dialogpt-gpt2-Ties",
    "DialoGPT gpt2 Ties DialoGPT Heavy", "DialoGPT gpt2 Ties gpt2 Heavy"
]

# Scores
perplexity_scores = [22.73, 22264.01, 7867.72, 2000.73, 33.71, 33.67, 42.91]
bleu_scores = [0.0204, 0.0017, 0.0001, 0.0027, 0.0199, 0.0199, 0.0127]
rouge_1_scores = [0.0873, 0.0033, 0.0006, 0.0105, 0.0751, 0.0758, 0.0519]
rouge_2_scores = [0.0179, 0.0000, 0.0000, 0.0020, 0.0187, 0.0187, 0.0085]
rouge_L_scores = [0.0811, 0.0030, 0.0006, 0.0099, 0.0711, 0.0718, 0.0477]

# Create subplots: 3 rows, 2 columns
fig, axs = plt.subplots(3, 2, figsize=(15, 16))

axs[0, 0].barh(model_names, perplexity_scores, color='skyblue')
axs[0, 0].set_title("Perplexity Scores")
axs[0, 0].set_xlabel("Perplexity")
axs[0, 0].set_xscale('log')

axs[0, 1].barh(model_names, bleu_scores, color='lightgreen')
axs[0, 1].set_title("BLEU Scores")
axs[0, 1].set_xlabel("BLEU")

axs[1, 0].barh(model_names, rouge_1_scores, color='salmon')
axs[1, 0].set_title("ROUGE-1 Scores")
axs[1, 0].set_xlabel("ROUGE-1")

axs[1, 1].barh(model_names, rouge_2_scores, color='lightcoral')
axs[1, 1].set_title("ROUGE-2 Scores")
axs[1, 1].set_xlabel("ROUGE-2")

axs[2, 0].barh(model_names, rouge_L_scores, color='mediumpurple')
axs[2, 0].set_title("ROUGE-L Scores")
axs[2, 0].set_xlabel("ROUGE-L")

# Hide the unused subplot (bottom right)
fig.delaxes(axs[2, 1])

plt.tight_layout()
plt.show()